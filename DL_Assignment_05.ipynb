{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhmJI3DDTKweGpFdZQ9H2I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maheshkumar145/DL_Theory/blob/main/DL_Assignment_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.\tWhy would you want to use the Data API?**\n",
        "\n",
        "**Ans:** API (Application Programming Interface)  a software intermediary that allows two applications to talk to each other.APIs are useful for more than accessing information for data-driven decision-making. You can also build programs that run searches on the data the server is hosting and transform that information into a different, usable format."
      ],
      "metadata": {
        "id": "GI7SV_XOlU9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.\tWhat are the benefits of splitting a large dataset into multiple files?**\n",
        "\n",
        "**Ans:**Splitting data is typically done to avoid overfitting. That is an instance where a machine learning model fits its training data too well and fails to fit with additional data. The original data in a machine learning model is typically taken and split into three or four sets.\n",
        "\n",
        "It’s also simpler to manipulate thousands of small files rather than one huge file; for example, it’s easier to split the data into multiple subsets.\n",
        "\n",
        "Lastly, if the data is split across multiple files spread across multiple servers, it is possible to download several files from different servers simultaneously, which improves the bandwidth usage."
      ],
      "metadata": {
        "id": "t2wPoobLlTJi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.\tDuring training, how can you tell that your input pipeline is the bottleneck? What can you do to fix it?**\n",
        "\n",
        "**Ans:** A CPU bottleneck occurs when the GPU resource is under utilized as a result of one, or more of the CPUs, having reached maximum utilization. In this situation, the GPU will be partially idle while it waits for the CPU to pass in training data. This is an undesired state.\n",
        "\n",
        "Technically, a CPU bottleneck generally occurs when the ratio between the “amount” of data pre-processing, which is performed on the CPU, and the “amount” of compute performed by the model on the GPU, is greater that the ratio between the overall CPU compute capacity and the overall GPU compute capacity.\n",
        "\n",
        "For example, if both your CPU cores and GPU are maximally utilized, and then you upgrade to a more powerful GPU, or downgrade to a system with fewer CPU cores, your training runtime performance will become CPU bound."
      ],
      "metadata": {
        "id": "btqvI_mylRoy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.\tCan you save any binary data to a TF Record file, or only serialized protocol buffers?**\n",
        "\n",
        "**Ans:** Yes, TFRecord format is a simple format for storing a sequence of binary records. Protocol buffers are a cross-platform, cross-language library for efficient serialization of structured data"
      ],
      "metadata": {
        "id": "55Hwq9OxlQEy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.\tWhy would you go through the hassle of converting all your data to the Example protobuf format? Why not use your own protobuf definition?**\n",
        "\n",
        "**Ans:** The Protobuf is optimized to be faster by removing many responsibilities usually done by data formats and making it focus only on the ability to serialize and deserialize data as fast as possible."
      ],
      "metadata": {
        "id": "sUcRZuZ0lOBq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.\tWhen using TFRecords, when would you want to activate compression? Why not do it systematically?**\n",
        "\n",
        "**Ans:** Compression is used when we want to export a file to TFRecords by using GZIP compression.If we do systematically ,we have to tell Tensorflow shape and property of the array or data."
      ],
      "metadata": {
        "id": "hCspI7WUlL7S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7.\tData can be preprocessed directly when writing the data files, or within the tf.data pipeline, or in preprocessing layers within your model, or using TF Transform. Can you list a few pros and cons of each option?**\n",
        "\n",
        "**Ans:**\n",
        "\n",
        "**1.Preprocessed directly when writing the data files:**\n",
        "\n",
        "**Pros:**\n",
        "\n",
        "* If you preprocess the data when creating the data files, the training script \n",
        "will run faster, since it will not have to perform preprocessing on the fly.\n",
        "\n",
        "* In some cases, the preprocessed data will also be much smaller than the original data, so you can save some space and speed up downloads.\n",
        "\n",
        "* It may also be helpful to materialize the preprocessed data, for example to inspect it or archive it.\n",
        "\n",
        "**Cons:**\n",
        "\n",
        "* it’s not easy to experiment with various preprocessing logics if you need to generate a preprocessed dataset for each variant.\n",
        "\n",
        "* if you want to perform data augmentation, you have to materialize many variants of your dataset, which will use a large amount of disk space and take a lot of time to generate. -Lastly, the trained model will expect preprocessed data, so you will have to add preprocessing code in your application before it calls the model.\n",
        "\n",
        "**2.TF.data pipeline:**\n",
        "\n",
        "* If the data is preprocessed with the tf.data pipeline, it’s much easier to tweak the preprocessing logic and apply data augmentation.\n",
        "\n",
        "* Also, tf.data makes it easy to build highly efficient preprocessing pipelines (e.g., with multithreading and prefetching).\n",
        "\n",
        "* However, preprocessing the data this way will slow down training.\n",
        "\n",
        "* Moreover, each training instance will be preprocessed once per epoch rather than just once if the data was preprocessed when creating the data files. Lastly, the trained model will still expect preprocessed data.\n",
        "\n",
        "**3.Preprocessing layers within your model:**\n",
        "\n",
        "* If you add preprocessing layers to your model, you will only have to write the preprocessing code once for both training and inference.\n",
        "\n",
        "* If your model needs to be deployed to many different platforms, you will not need to write the preprocessing code multiple times.\n",
        "\n",
        "* Plus, you will not run the risk of using the wrong preprocessing logic for your model, since it will be part of the model.\n",
        "\n",
        "* On the downside, preprocessing the data will slow down training, and each training instance will be preprocessed once per epoch.\n",
        "\n",
        "* Moreover, by default the preprocessing operations will run on the GPU for the current batch (you will not benefit from parallel preprocessing on the CPU, and prefetching).\n",
        "\n",
        "* Fortunately, the upcoming Keras preprocessing layers should be able to lift the preprocessing operations from the preprocessing layers and run them as part of the tf.data pipeline, so you will benefit from multithreaded execution on the CPU and prefetching.\n",
        "\n",
        "\n",
        "**4.TF Transform:**\n",
        "\n",
        "* Lastly, using TF Transform for preprocessing gives you many of the benefits from the previous options: the preprocessed data is materialized, each instance is preprocessed just once (speeding up training), and preprocessing layers get generated automatically so you only need to write the preprocessing code once.\n",
        "\n",
        "* The main drawback is the fact that you need to learn how to use this tool.\n",
        "\n"
      ],
      "metadata": {
        "id": "qp42vb4clKF0"
      }
    }
  ]
}
